model_configuration:
  model_name: GSAI-ML/LLaDA-8B-Instruct
  tokenizer_name: GSAI-ML/LLaDA-8B-Instruct
  torch_dtype: bfloat16
  device_map: cuda
  trust_remote_code: true
  steps: 300
  gen_length: 300
  temperature: 0.5
  remasking: random
  model_specific_arguments:
    cfg_scale: 0.0
    mask_id: 126336
    block_length: 25

watermark_type: Ours
watermark_config:
  delta: 2.0
  enforce_kl: false
  convolution_kernel: [-1]
  topk: 50
  n_iter: 1
  seeding_scheme: "sumhash"
  greenlist_type: "bernoulli"
  greenlist_params:
    gamma: 0.25

evaluation_config:
  num_samples: 200
  batch_size: 1
  evaluation_datasets:
    - path: data/WaterBench/2-1_longform_qa.jsonl
      user_prompt: "You are a helpful assistant, please answer the following question within 300 words:\n{context}\n{input}"
      chat: true
      max_length: 150
      min_length: 1
    - path: data/WaterBench/2-2_finance_qa.jsonl
      user_prompt: "You are a helpful assistant, please answer the following question with financial knowledge within 300 words:\n{context}\n{input}"
      chat: true
      max_length: 150
      min_length: 1
    - path: data/WaterBench/5-1_alpacafarm.jsonl
      user_prompt: "You are a helpful assistant, please answer the following instruction: \n{context}\n{input}"
      chat: true
      max_length: 150
      min_length: 1
  tqdm: true
